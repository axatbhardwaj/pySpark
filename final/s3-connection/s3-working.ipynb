{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW2cn5V_nJmW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8K2vNMVns_W"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tar xf spark-3.4.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.595/aws-java-sdk-bundle-1.12.595.jar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv aws-java-sdk-bundle-1.12.595.jar /content/spark-3.4.1-bin-hadoop3/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv hadoop-aws-3.3.4.jar /content/spark-3.4.1-bin-hadoop3/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk\" #change this accordingly\n",
        "# os.environ[\"SPARK_HOME\"] = \"/home/axat/personal/pySpark/s3-connection/spark-3.4.1-bin-hadoop3\" #chnage this accordingly\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \n",
        "\n",
        "# Specify the Hadoop and AWS configurations\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.5,com.amazonaws:aws-java-sdk-bundle:1.12.595\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "# Read CSV from S3\n",
        "df = spark.read.csv('s3a://deepak-test-infrablok/tested.csv', inferSchema=True)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRxllwjvnucl"
      },
      "outputs": [],
      "source": [
        "!tar xf spark-3.4.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXpfwISypFX2"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXzEeL74pUcE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToPWNLKFptMm"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qahSS34qp8Qh",
        "outputId": "063eb636-9352-48f1-965c-de4d77e9729f"
      },
      "outputs": [],
      "source": [
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5AsubjKwHX_"
      },
      "outputs": [],
      "source": [
        "!wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvg1SYzqzoqb"
      },
      "outputs": [],
      "source": [
        "!wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.595/aws-java-sdk-bundle-1.12.595.jar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF3SmxeJwXH1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTapUn36zs9V"
      },
      "outputs": [],
      "source": [
        "!mv aws-java-sdk-bundle-1.12.595.jar /content/spark-3.4.1-bin-hadoop3/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd2mPnJtwX-J"
      },
      "outputs": [],
      "source": [
        "!mv hadoop-aws-3.3.4.jar /content/spark-3.4.1-bin-hadoop3/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ewRGQxPIkg"
      },
      "outputs": [],
      "source": [
        "! export AWS_ACCESS_KEY_ID=\n",
        "! export AWS_SECRET_ACCESS_KEY="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnKONPMWqKOc",
        "outputId": "f1c3cbfb-6c68-4bfe-8d27-f48acc926d7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/axat/work/pySpark/pyspark-s3/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Specify the Hadoop and AWS configurations\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.5,com.amazonaws:aws-java-sdk-bundle:1.12.595\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "# Read CSV from S3\n",
        "df = spark.read.csv('s3a://deepak-test-infrablok/tested.csv', inferSchema=True)\n",
        "df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY_nRoA3oLni"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8r6LPCN0QEL"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
